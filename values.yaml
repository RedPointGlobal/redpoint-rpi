global:
  application:
    name: redpoint-interaction
    version: 7

  deployment:
    # Specify the cloud environment where the deployment will run.
    # Supported options: azure, amazon, google, selfhosted
    # "selfhosted" is intended for on-premise or non-cloud based kubernetes deployments.
    platform: amazon

    # List of images
    images:
      interactionapi: rg1acrpub.azurecr.io/docker/redpointglobal/releases/rpi-interactionapi
      integrationapi: rg1acrpub.azurecr.io/docker/redpointglobal/releases/rpi-integrationapi
      executionservice: rg1acrpub.azurecr.io/docker/redpointglobal/releases/rpi-executionservice
      nodemanager: rg1acrpub.azurecr.io/docker/redpointglobal/releases/rpi-nodemanager
      callbackapi: rg1acrpub.azurecr.io/docker/redpointglobal/releases/rpi-callbackapi
      deploymentapi: rg1acrpub.azurecr.io/docker/redpointglobal/releases/rpi-deploymentapi
      # If not using RPI Realtime, these are not needed
      realtimeapi: rg1acrpub.azurecr.io/docker/redpointglobal/releases/rpi-realtimeapi
      queuereader: rg1acrpub.azurecr.io/docker/redpointglobal/releases/rpi-queuereader
      # If using external (BYO) Redis or RabbitMQ servers, these are not needed.
      rabbitmq: rg1acrpub.azurecr.io/docker/redpointglobal/releases/rabbitmq
      rediscache: rg1acrpub.azurecr.io/docker/redpointglobal/releases/rediscache
      # If using external (BYO) ingress controller, this is not needed
      ingress_controller: rg1acrpub.azurecr.io/docker/redpointglobal/releases/nginx-ingress-controller
    
      # Specific version of RPI to be deployed, where <major.minor>.<year>-<MMDD>-<HHMM>
      tag: 7.6.2025-0522-1016
      # Pull the image only if it's not already present on the node
      imagepullPolicy: IfNotPresent
      # Use an image pull secret for private registry authentication
      imagePullSecret:
        enabled: true
        # Name of the Kubernetes secret containing registry credentials
        name: redpoint-rpi

# ==========================
databases:
  # Operational database providers 
  operational:
    # Supported options:
    # - sqlserver (can be hosted on Azure, Google Cloud, AWS RDS)
    # - postgresql (can be hosted on Azure, Google Cloud, AWS RDS)
    # - sqlserveronvm (SQL Server on a Virtual Machine)
    provider: sqlserver
    # The hostname or IP address of the database server
    server_host: <my-database-server-host>
    # The username for accessing the database
    server_username: <my-database-server-username>
    # The password for the given username
    server_password: <my-database-server-password>
    # The name of the operational database
    pulse_database_name: <my-pulse-database-name>
    # The name of the logging database
    pulse_logging_database_name: <my-logging-database-name>
    # The schema to be used within the database
    databaseSchema: dbo
    # When set to true, appends `Encrypt=True` to the SQL Server connection string.
    # If your SQL Server enforces encryption (e.g., Force Encryption is enabled) this must be set to true
    encrypt: true

  # Datawarehouse Providers
  # This section only applies if your datawarehouse is one of Redshift, BigQuery or snowflake. 
  # Redshift and BigQuery use ODBC drivers, which require a configuration file to be included in the containers. 
  # The details you provide are used to configure the Data Source Name (DSN). 
  # After deployment, the connection string for your Redshift or BigQuery data warehouse would look 
  # like this: ```dsn=redshift``` or ```dsn=bigquery``` 
  datawarehouse:
    enabled: true
    # Only update the fields relevant to your chosen option and leave the others unchanged.
    # For example, if you're using Redshift, update the Redshift fields and leave Snowflake, BigQuery fields as-is.
    # Supported options: (redshift, bigquery, snowflake)
    # In RPI use the provider name as the DSN name
    provider: redshift
    # ==========================
    # Configuration for Redshift
    redshift:
      # The endpoint for your Redshift cluster
      server: <my-redshift-endpoint>
      # Default port for Redshift
      port: 5439
      # Name of the Redshift database
      database: <my-redshift-database-name>
      # Redshift database username
      username: <my-redshift-username>
      # Redshift database password
      password: <my-redshift-password>
      
    # ==========================
    # Configuration for BigQuery
    bigquery:
      # The Google Cloud Project ID
      projectId: my_google_project_id
      # SQL dialect option (1 = Standard SQL)
      sqlDialect: 1
      # OAuth mechanism (0 = Service Account, 1 = User Credentials)
      OAuthMechanism: 0 
      credentialsType: serviceAccount
      # Service account email
      serviceAccountEmail: my-google-svs-account@my-project.iam.gserviceaccount.com 
      # Create a ConfigMap containing the service account JSON credentials file 
      # and provide the name below
      configMapName: my-google-svs-account 
      # Name of the service account JSON key
      keyName: my-google-svs-account.json
      # Path to the service account JSON key file
      ConfigMapFilePath: /app/google-creds
      # AllowLargeResults: When set to 1, the driver allows for result sets in responses to be larger than 128 MB.
      allowLargeResults: 0
      # LargeResultsDataSetId: DatasetId to store temporary tables created.  This is a required setting if AllowLargeResults is set to 1.
      largeResultsDataSetId: _bqodbc_temp_tables
      # LargeResultsTempTableExpirationTime: Time in milliseconds before the temporary tables created expire.  
      # This is a required setting if AllowLargeResults is set to 1.
      largeResultsTempTableExpirationTime: "3600000"

    # ==========================
    # Snowflake Configuration
    #
    # This section configures access to a Snowflake data warehouse using RSA key pair authentication,
    # which provides enhanced security compared to basic authentication (username and password).
    #
    # Apply this configuration if your Snowflake instance requires RSA key-based authentication.
    # ********** This feature will be available starting with the upcoming RPI 7.7 release ****************
    snowflake:
      # Name of the ConfigMap storing Snowflake credentials
      ConfigMapName: snowflake-creds
      # Name of the RSA key for authentication
      keyName: my-snowflake-rsakey.p8
      # Mount path for the credentials in the container
      ConfigMapFilePath: /app/snowflake-creds
      # In RPI, use the following format for the Snowflake connection string:
      # User=<snowflakeUser>;Db=<snowflakeDB>;host=<snowflakeHost>;Account=<snowflake account>;AUTHENTICATOR=snowflake_jwt;PRIVATE_KEY_FILE=/app/snowflake-creds/my_snowflake_rsa_key.p8;PRIVATE_KEY_PWD=<privateKeyPassword>

# ==========================
# Certain RPI functionality (e.g., secret managers, Azure and GCP plugins) is able to make use of 
# cloud provider identity to authenticate with cloud services.
cloudIdentity:
  enabled: false
  # Authentication method for accessing cloud services.
  # Supported options: Azure, Google, Amazon
  provider: Amazon
  secretsManagement:
    enabled: false
    # Specify how to reference required secrets, such as database passwords and connection strings.
    # Supported options: 'kubernetes' or 'keyvault'
    #
    # - Use 'kubernetes' to have the Helm chart automatically create the secrets within your Kubernetes cluster.
    # - Use 'keyvault' to disable Kubernetes secret creation and pull secrets from an external key vault.
    #
    # For more information, refer to the ```Configure Secrets Management``` in the README.md
    secretsProvider: kubernetes
    # If secretsProvider is kubernetes - Let the Helm chart automatically create the secret
    autoCreateSecrets: true
    # Name of the kubernetes secret to be created
    secretName: redpoint-rpi-secrets
    # Use Key Vault for system configuration passwords
    UseForConfigPasswords: true
    UseForAppSettings: true
    # Interval in seconds to reload configuration from Key Vault
    ConfigurationReloadIntervalSeconds: 30
  azureSettings:
    # Supported options: workloadIdentity
    credentialsType: workloadIdentity
    managedIdentityClientId: your_managed_identity_client_id
    UseADTokenForDatabaseConnection: false
    # If your secretsProvider is keyvault
    vaultUri: https://myvault.vault.azure.net/
    appSettingsVaultUri: https://myvault.vault.azure.net/
  googleSettings:
    credentialsType: serviceAccount
    # Create a ConfigMap containing the service account JSON credentials file 
    # and provide the name below
    configMapName: my-google-svs-account
    keyName: my-google-svs-account.json
    ConfigMapFilePath: /app/google-creds
    serviceAccountEmail: my-google-svs-account@my-project.iam.gserviceaccount.com
    # Name of google project
    projectId: your_google_project_id
  amazonSettings:
    credentialsType: accessKey
    accessKeyId: <my-aws-iam-access-key>
    secretAccessKey: <my-aws-iam-secret-access-key>
    region: us-east-1  

# ==========================      
storage:
  # Configure storage for the RPI File Output and Data Management (RPDM) upload directories.
  # This chart is intentionally storage agnostic and does not enforce any specific storage solution.
  # You are responsible for creating the appropriate storage configuration based on your cloud providerâ€™s requirements.
  # Provide the name of the Persistent Volume Claim (PVC) to be used below.
  # If storage is not needed, set 'enabled' to false.
  persistentVolumeClaims:
    FileOutputDirectory:
      enabled: false
      claimName: rpifileoutputdir
      mountPath: /rpifileoutputdir
    Plugins:
      enabled: false
      claimName: realtimeplugins
      mountPath: /realtimeplugins
    DataManagementUploadDirectory:
      enabled: false
      claimName: rpdmuploaddirectory
      mountPath: /rpdmuploaddirectory

# ==========================
realtimeapi:
  # If disabled, the realtime pod will not be deployed
  enabled: true
  # Number of pod replicas to deploy
  replicas: 1
  name: rpi-realtimeapi
  serviceAccount: 
    enabled: true
  # Ensure this matches your target RPI client ID.
  rpiClientID: 00000000-0000-0000-0000-000000000000
  # The default authentication method for the Realtime API is an authentication 
  # token in the header of the call to the API endpoint
  authentication:
    # Supported options are 
    # - basic
    # - oauth
    type: basic
    # With basic authentication, methods protected by the Standard role do not require an RPIAuthKey 
    # authentication token whereas the rest require a token.
    basic:
      authToken: 00000000-0000-0000-0000-000000000000
      standard: false
      forms: true
      listenerQueue: true
      recommendations: true
    # Once OAuth authentication is enabled, it takes precedence over the usage of static token authentication.
    # An RPI Realtime authentication database must be made available, and 
    # Refer to https://docs.redpointglobal.com/rpi/rpi-realtime-authentication
    oauth:
      databaseName: RPIRealtimeCore
      accessTokenLifetimeSeconds: 360
      refreshTokenLifetimeSeconds: 360
  # Expose the Realtime API Swagger page for API exploration and testing.
  enableHelpPages: true 
  enableEventListening: true
  realtimeProcessingEnabled: true
  CORSOrigins: ["*"]
  ThresholdBetweenSiteVisitsMinutes: 30
  CacheWebFormData: true
  decisionCacheDuration: 60
  enableAuditMetricsInHeaders: true
  # If set, profile data will be sent via queue to RPI
  cacheOutputQueueEnabled: true
  RealtimeServerCookieEnabled: false
  ThresholdBetweenSiteVisitsMinutes: 120
  ThresholdBetweenPageVisitsMinutes: 1
  CacheOutputCollectIPAddress: true
  CacheWebFormData: false
  HashVisitorID: false
  EventListeningLocalCacheDuration: 60
  RealtimeServerCookieHttpOnly: false
  RealtimeServerCookieName: rg-visitor
  RealtimeServerCookieExpires: 60
  RealtimeServerCookieDomain: ""
  cacheProvider:
    # This section defines the settings for the real-time cache provider.
    # Update only the fields relevant to your chosen provider and leave the others unchanged.
    # Set to true to enable real-time cache configuration.
    enabled: true  
    # Choose your cache provider
    # Supported options: (mongodb, azureredis, redis, inMemorySql, googlebigtable).
    provider: mongodb
    # If using MongoDB as the provider
    mongodb:
      # Provide the MongoDB connection string.
      connectionString: mongodb://<my_username>:<my_password>@myserver.mongodb.net:27017/Pulse?authSource=admin&ssl=true
      databaseName: <my-realtime-cache-db>
      collectionName: <my-realtime-cache-collection>

    # If using Azure Redis as the provider
    azureredis: 
      # Provide the Azure Redis server host URL.
      connectionstring: redis://<my-redis-name>.redis.cache.windows.net:6380?ssl=true&password=<my-access-key>

    # If using Redis as the provider
    redis:
      # Set to true to deploy an internal Redis container.
      # Set to false if using an external (BYO) Redis instance.
      internal: false
      replicas: 2
      # Redis connection string used when 'internal' is false.
      # Format: <hostname>:<port>,ssl=<true|false>,abortConnect=<true|false>,user=<username>,password=<password>
      connectionstring: <my-redis-hostname>:6379,ssl=True,abortConnect=false,user=<my-username>,password=<my-username>
      
    # In-memory SQL cache configuration
    inMemorySql:
      # Hostname or IP address of the cache database server
      serverHost: "<my-cache-database-server-host>"
      # Name of the in-memory SQL database
      databaseName: "<my-cache-database-name>"
      # Credentials for database access
      username: "<my-cache-database-username>"
      password: "<my-cache-database-password>"
    
    googlebigtable:
      projectId: <my-bigtable-project-id>
      instanceId: <my-bigtable-instance-id>
      
  queueProvider:
    # This section defines the settings for the real-time message queue provider.
    # Set to true to enable real-time queue configuration
    enabled: true  
    # Choose your message queue provider
    # Supported options: (amazonsqs, azureservicebus, googlepubsub, azureeventhubs, rabbitmq)
    provider: amazonsqs  
    queueNames: 
      # Queues required by the RPI Realtime service
      # Ensure that you replace these values with the actual names of the queues 
      # that are allocated for your specific environment.
      formQueuePath: RPIWebFormSubmission
      eventsQueuePath: RPIWebEvents
      cacheOutputQueuePath: RPIWebCacheData
      recommendationsQueuePath: RPIWebRecommendations
      # Overrides the standard queue settings for event stream processing
      listenerQueuePath: RPIQueueListener
      callbackServiceQueuePath: RPICallbackApiQueue
    # Update only the fields relevant to your chosen provider and leave the others unchanged.
    # If using AWS SQS as the provider
    amazonsqs:
      # Specify the authentication method for accessing Amazon SQS.
      # Supported options:
      #   - accessKey: Use static AWS access key and secret key (configure these under the cloudIdentity.amazonSettings section above).
      credentialsType: accessKey

    # If using Azure Service Bus as the queue provider
    azureservicebus:
      # Provide the Azure Service Bus connection string
      connectionstring: Endpoint=sb://<my-service-bus-namespace>.servicebus.windows.net/;SharedAccessKeyName=<my-policy-name>;SharedAccessKey=<my-policy-key>

    # If using Google Pub/Sub as the queue provider
    googlepubsub:
      # Provide the Google Cloud project ID
      projectId: <my-google-project-id>

    # If using Azure Event Hubs as the queue provider
    azureeventhubs:
      # Provide the Azure Event Hub connection string
      connectionstring: Endpoint=sb://<my-event-hubs-namespace>.servicebus.windows.net/;SharedAccessKeyName=<my-policy-name>;SharedAccessKey=<my-policy-key>;EntityPath=<my-event-hub-name>
      eventHubName: RPIQueueListener

    # If using RabbitMQ as the queue provider
    rabbitmq:
      # Set to true to deploy an internal Redis container.
      # Set to false if using an external (BYO) Redis instance.
      internal: false
      # Number of RabbitMQ replicas (only applicable when 'internal' is true)
      replicas: 2
      # RabbitMQ hostname or service name
      hostname: realtime-rabbitmq
      # Virtual host to use within RabbitMQ
      virtualHost: "/"
      # Credentials for RabbitMQ access
      username: redpointdev
      password: <my-secure-password>
    
    # If using Amazon Managed Kafka (MSK) as the queue provider
    amazonmsk:
      region: <my-aws-region>
      accessKeyId: <my-aws-iam-access-key>
      secretAccessKey: <my-aws-iam-secret-access-key>
      bootstrapServers: <my-server1.c2.kafka.us-east-1.amazonaws.com:9198>
      Acks: None
      CompressionType: Snappy
      MaxRetryAttempt: 10
      BatchSize: "1000000"
      LingerTime: 0
      UseAwsMsk: True
  dataMaps:
    visitorProfile:
      DaysToPersist: 365
      Cache: Default
      DaysToPersist: 365
      CompressData: true
    visitorHistory:
      DaysToPersist: 365
      Cache: Default
      DaysToPersist: 365
      CompressData: true
    nonVisitorData:
      DaysToPersist: 365
      Cache: Default
      DaysToPersist: 365
      CompressData: true
    productRecommendation:
      DaysToPersist: 365
      Cache: Default
      DaysToPersist: 365
      CompressData: true
    offerHistory:
      DaysToPersist: 365
      Cache: Default
      DaysToPersist: 365
      CompressData: true
    messageHistory:
      DaysToPersist: 365
      Cache: Default
      DaysToPersist: 365
      CompressData: true
  # Default resource requests and limits per deployment.
  # These values are designed to fit 8 deployments on a single node with 8 vCPUs and 16 GB RAM.
  # Treat these as starting points and adjust accordingly to meet your utilization and performance requirements.
  resources:
    requests:
      cpu: 500m
      memory: 750Mi
    limits:
      cpu: 875m
      memory: 2048Mi
  service:
    # The port on which the Kubernetes services will be exposed.
    # This can be customized to align with the customer's internal port conventions.
    port: 80
  # Optional custom labels to apply to the Deployment and Pod metadata.
  # These will be added in addition to the default labels.
  # Example: environment: prod, team: marketing
  customLabels: 
    environment: "prod"
    team: "marketing"
  customAnnotations:
    my-custom-annotation: "my-value"
  customMetrics:
    # Enable or disable custom Prometheus metrics scraping for this service
    enabled: false
    # When enabled, the following annotation will be added to the pod metadata
    # to allow Prometheus scrape the /metrics endpoint
    prometheus_scrape: false
  # Set the minimum log level for application logging.
  # Supported options: Critical, Error, Warning, Information, Trace, Debug
  logging:
    default: Error
    database: Error
    rpiTrace: Error
    rpiError: Error
  # Enable Horizontal autoscaling
  autoscaling:
    enabled: true
    # Options: "keda" or "hpa".
    # 
    # - "keda": Enables KEDA (Kubernetes-based Event Driven Autoscaling), allowing scaling based on RPI custom metrics from
    #   Prometheus. This option assumes that KEDA is already installed in your cluster.
    #   If not, you can install it with Documentation: https://keda.sh/docs/latest/deploy/
    #
    # - "hpa": Enables the standard Kubernetes Horizontal Pod Autoscaler, which scales pods based on CPU and/or memory 
    #   usage metrics. This is natively supported by Kubernetes and does not require additional installation.
    #
    # Choose the appropriate option based on your scaling requirements. If you need scaling based on RPI custom metrics, use "keda".
    # For resource-based scaling, use "hpa".
    type: hpa
    minReplicas: 1
    maxReplicas: 5
    targetCPUUtilizationPercentage: 80
    # targetMemoryUtilizationPercentage: 80
    kedaScaledObject:
      # The Prometheus server address where metrics are queried.
      serverAddress: <my-prometheus-query-endpoint>
      # The name of the RPI custom metric KEDA will use to trigger scaling.
      metricName: <my-metrics-name>
      # The Prometheus query that fetches the metric value for scaling decisions.
      # This example filters the metric `execution_max_thread_count` based on:
      # - Namespace: "redpoint-rpi"
      # - Application: "rpi-executionservice"
      # - Azure Workload Identity: "true"
      #
      # Update these labels if your namespace, app, or identity settings are different.
      query: count(execution_tasks_executing_count{kubernetes_namespace="redpoint-rpi", app="rpi-execution-service", azure_workload_identity_use="true"})
      # Threshold for the value configured for `RPIExecution__MaxThreadsPerExecutionService`
      # It defines the limit on how many work items an execution service can take on.
      threshold: "90"
      # Frequency (in seconds) at which KEDA checks the metric value.
      pollingInterval: 15
      # Minimum number of pods to maintain, even if the load is low
      minReplicaCount: 2
      # Maximum number of pods to scale up to when the load increases.
      maxReplicaCount: 5
  # Liveness probe checks if the app is still running. 
  # If it fails, the pod will be restarted.
  livenessProbe:
    periodSeconds: 10
    failureThreshold: 3
    timeoutSeconds: 2
  # Readiness probe checks if the app is ready to receive traffic. 
  # If it fails, traffic is paused.
  readinessProbe:
    initialDelaySeconds: 5
    periodSeconds: 10
    failureThreshold: 3
    timeoutSeconds: 2
  # Startup probe checks if the app has finished starting.
  # Until it passes, liveness and readiness probes are ignored.
  startupProbe:
    failureThreshold: 60
    periodSeconds: 10
    timeoutSeconds: 2

# ==========================
callbackapi:
  # If disabled, the callbackapi pod will not be deployed
  enabled: true
  # Number of pod replicas to deploy
  replicas: 1
  name: rpi-callbackapi
  channelLabel: SendGrid
  serviceAccount: 
    enabled: true 
  # Default resource requests and limits per deployment.
  # These values are designed to fit 8 deployments on a single node with 8 vCPUs and 16 GB RAM.
  # Treat these as starting points and adjust accordingly to meet your utilization and performance requirements.
  resources:
    requests:
      cpu: 500m
      memory: 750Mi
    limits:
      cpu: 875m
      memory: 2048Mi
  service:
    # The port on which the Kubernetes services will be exposed.
    # This can be customized to align with the customer's internal port conventions.
    port: 80
  # Optional custom labels to apply to the Deployment and Pod metadata.
  # These will be added in addition to the default labels.
  # Example: environment: prod, team: marketing
  customLabels: 
    environment: "prod"
    team: "marketing"
  customAnnotations:
    my-custom-annotation: "my-value"
  customMetrics:
    # Enable or disable custom Prometheus metrics scraping for this service
    enabled: false
    # When enabled, the following annotation will be added to the pod metadata
    # to allow Prometheus scrape the /metrics endpoint
    prometheus_scrape: false
  # Set the minimum log level for application logging.
  # Supported options: Critical, Error, Warning, Information, Trace, Debug
  logging:
    default: Error
    database: Error
    rpiTrace: Error
    rpiError: Error
  # Enable Horizontal autoscaling
  autoscaling:
    enabled: true
    # Options: "keda" or "hpa".
    # 
    # - "keda": Enables KEDA (Kubernetes-based Event Driven Autoscaling), allowing scaling based on RPI custom metrics from
    #   Prometheus. This option assumes that KEDA is already installed in your cluster.
    #   If not, you can install it with Documentation: https://keda.sh/docs/latest/deploy/
    #
    # - "hpa": Enables the standard Kubernetes Horizontal Pod Autoscaler, which scales pods based on CPU and/or memory 
    #   usage metrics. This is natively supported by Kubernetes and does not require additional installation.
    #
    # Choose the appropriate option based on your scaling requirements. If you need scaling based on RPI custom metrics, use "keda".
    # For resource-based scaling, use "hpa".
    type: hpa
    minReplicas: 1
    maxReplicas: 5
    targetCPUUtilizationPercentage: 80
    # targetMemoryUtilizationPercentage: 80
    kedaScaledObject:
      # The Prometheus server address where metrics are queried.
      serverAddress: <my-prometheus-query-endpoint>
      # The name of the RPI custom metric KEDA will use to trigger scaling.
      metricName: <my-metrics-name>
      # The Prometheus query that fetches the metric value for scaling decisions.
      # This example filters the metric `execution_max_thread_count` based on:
      # - Namespace: "redpoint-rpi"
      # - Application: "rpi-executionservice"
      # - Azure Workload Identity: "true"
      #
      # Update these labels if your namespace, app, or identity settings are different.
      query: count(execution_max_thread_count{kubernetes_namespace="redpoint-rpi",app="rpi-executionservice",azure_workload_identity_use="true"})
      # Threshold for the value configured for `RPIExecution__MaxThreadsPerExecutionService`
      # It defines the limit on how many work items an execution service can take on.
      threshold: "90"
      # Frequency (in seconds) at which KEDA checks the metric value.
      pollingInterval: 15
      # Minimum number of pods to maintain, even if the load is low
      minReplicaCount: 2
      # Maximum number of pods to scale up to when the load increases.
      maxReplicaCount: 5
  # Liveness probe checks if the app is still running. 
  # If it fails, the pod will be restarted.
  livenessProbe:
    periodSeconds: 10
    failureThreshold: 3
    timeoutSeconds: 2
  # Readiness probe checks if the app is ready to receive traffic. 
  # If it fails, traffic is paused.
  readinessProbe:
    initialDelaySeconds: 5
    periodSeconds: 10
    failureThreshold: 3
    timeoutSeconds: 2
  # Startup probe checks if the app has finished starting.
  # Until it passes, liveness and readiness probes are ignored.
  startupProbe:
    failureThreshold: 60
    periodSeconds: 10
    timeoutSeconds: 2


# ==========================      
executionservice:
  # If disabled, the executionservice pod will not be deployed
  enabled: true
  # Number of pod replicas to deploy
  replicas: 1
  name: rpi-executionservice
  RPIExecution__QueueListener__IsEnabled: false 
  RPIExecution_MaxThreadsPerExecutionService: 100
  enableRPIAuthentication: true
  serviceAccount: 
    enabled: true 
  # Default resource requests and limits per deployment.
  # These values are designed to fit 8 deployments on a single node with 8 vCPUs and 16 GB RAM.
  # Treat these as starting points and adjust accordingly to meet your utilization and performance requirements.
  resources:
    requests:
      cpu: 500m
      memory: 750Mi
    limits:
      cpu: 875m
      memory: 2048Mi
  service:
    # The port on which the Kubernetes services will be exposed.
    # This can be customized to align with the customer's internal port conventions.
    port: 80
  # Optional custom labels to apply to the Deployment and Pod metadata.
  # These will be added in addition to the default labels.
  # Example: environment: prod, team: marketing
  customLabels: 
    environment: "prod"
    team: "marketing"
  customAnnotations:
    my-custom-annotation: "my-value"
  customMetrics:
    # Enable or disable custom Prometheus metrics scraping for this service
    enabled: false
    # When enabled, the following annotation will be added to the pod metadata
    # to allow Prometheus scrape the /metrics endpoint
    prometheus_scrape: false
  # Set the minimum log level for application logging.
  # Supported options: Critical, Error, Warning, Information, Trace, Debug
  logging:
    default: Error
    database: Error
    rpiTrace: Error
    rpiError: Error
  # Enable Horizontal autoscaling
  autoscaling:
    enabled: true
    # Options: "keda" or "hpa".
    # 
    # - "keda": Enables KEDA (Kubernetes-based Event Driven Autoscaling), allowing scaling based on RPI custom metrics from
    #   Prometheus. This option assumes that KEDA is already installed in your cluster.
    #   If not, you can install it with Documentation: https://keda.sh/docs/latest/deploy/
    #
    # - "hpa": Enables the standard Kubernetes Horizontal Pod Autoscaler, which scales pods based on CPU and/or memory 
    #   usage metrics. This is natively supported by Kubernetes and does not require additional installation.
    #
    # Choose the appropriate option based on your scaling requirements. If you need scaling based on RPI custom metrics, use "keda".
    # For resource-based scaling, use "hpa".
    type: hpa
    minReplicas: 1
    maxReplicas: 5
    targetCPUUtilizationPercentage: 80
    # targetMemoryUtilizationPercentage: 80
    kedaScaledObject:
      # The Prometheus server address where metrics are queried.
      serverAddress: <my-prometheus-query-endpoint>
      # The name of the RPI custom metric KEDA will use to trigger scaling.
      metricName: <my-metrics-name>
      # The Prometheus query that fetches the metric value for scaling decisions.
      # This example filters the metric `execution_max_thread_count` based on:
      # - Namespace: "redpoint-rpi"
      # - Application: "rpi-executionservice"
      # - Azure Workload Identity: "true"
      #
      # Update these labels if your namespace, app, or identity settings are different.
      query: count(execution_tasks_executing_count{kubernetes_namespace="redpoint-rpi",app="rpi-executionservice",azure_workload_identity_use="true"})
      # Threshold for the value configured for `RPIExecution__MaxThreadsPerExecutionService`
      # It defines the limit on how many work items an execution service can take on.
      threshold: "90"
      # Frequency (in seconds) at which KEDA checks the metric value.
      pollingInterval: 15
      # Minimum number of pods to maintain, even if the load is low
      minReplicaCount: 2
      # Maximum number of pods to scale up to when the load increases.
      maxReplicaCount: 5
  # Liveness probe checks if the app is still running. 
  # If it fails, the pod will be restarted.
  livenessProbe:
    periodSeconds: 10
    failureThreshold: 3
    timeoutSeconds: 2
  # Readiness probe checks if the app is ready to receive traffic. 
  # If it fails, traffic is paused.
  readinessProbe:
    initialDelaySeconds: 5
    periodSeconds: 10
    failureThreshold: 3
    timeoutSeconds: 2
  # Startup probe checks if the app has finished starting.
  # Until it passes, liveness and readiness probes are ignored.
  startupProbe:
    failureThreshold: 60
    periodSeconds: 10
    timeoutSeconds: 2

# ========================== 
interactionapi: 
  # If disabled, the interactionapi pod will not be deployed
  enabled: true
  # Number of pod replicas to deploy
  replicas: 1
  name: rpi-interactionapi
  enableRPIAuthentication: true
  enableSwagger: true
  serviceAccount: 
    enabled: true 
  allowSavingLoginDetails: true
  alwaysShowClientsAtLogin: true
  # Default resource requests and limits per deployment.
  # These values are designed to fit 8 deployments on a single node with 8 vCPUs and 16 GB RAM.
  # Treat these as starting points and adjust accordingly to meet your utilization and performance requirements.
  resources:
    requests:
      cpu: 500m
      memory: 750Mi
    limits:
      cpu: 875m
      memory: 2048Mi
  service:
    # The port on which the Kubernetes services will be exposed.
    # This can be customized to align with the customer's internal port conventions.
    port: 80
  # Optional custom labels to apply to the Deployment and Pod metadata.
  # These will be added in addition to the default labels.
  # Example: environment: prod, team: marketing
  customLabels: 
    environment: "prod"
    team: "marketing"
  customAnnotations:
    my-custom-annotation: "my-value"
  customMetrics:
    # Enable or disable custom Prometheus metrics scraping for this service
    enabled: false
    # When enabled, the following annotation will be added to the pod metadata
    # to allow Prometheus scrape the /metrics endpoint
    prometheus_scrape: false
  # Set the minimum log level for application logging.
  # Supported options: Critical, Error, Warning, Information, Trace, Debug
  logging:
    default: Error
    database: Error
    rpiTrace: Error
    rpiError: Error
  # Enable Horizontal autoscaling
  autoscaling:
    enabled: true
    # Options: "keda" or "hpa".
    # 
    # - "keda": Enables KEDA (Kubernetes-based Event Driven Autoscaling), allowing scaling based on RPI custom metrics from
    #   Prometheus. This option assumes that KEDA is already installed in your cluster.
    #   If not, you can install it with Documentation: https://keda.sh/docs/latest/deploy/
    #
    # - "hpa": Enables the standard Kubernetes Horizontal Pod Autoscaler, which scales pods based on CPU and/or memory 
    #   usage metrics. This is natively supported by Kubernetes and does not require additional installation.
    #
    # Choose the appropriate option based on your scaling requirements. If you need scaling based on RPI custom metrics, use "keda".
    # For resource-based scaling, use "hpa".
    type: hpa
    minReplicas: 1
    maxReplicas: 5
    targetCPUUtilizationPercentage: 80
    # targetMemoryUtilizationPercentage: 80
    kedaScaledObject:
      # The Prometheus server address where metrics are queried.
      serverAddress: <my-prometheus-query-endpoint>
      # The name of the RPI custom metric KEDA will use to trigger scaling.
      metricName: <my-metrics-name>
      # The Prometheus query that fetches the metric value for scaling decisions.
      # This example filters the metric `execution_max_thread_count` based on:
      # - Namespace: "redpoint-rpi"
      # - Application: "rpi-executionservice"
      # - Azure Workload Identity: "true"
      #
      # Update these labels if your namespace, app, or identity settings are different.
      query: count(execution_max_thread_count{kubernetes_namespace="redpoint-rpi",app="rpi-executionservice",azure_workload_identity_use="true"})
      # Threshold for the value configured for `RPIExecution__MaxThreadsPerExecutionService`
      # It defines the limit on how many work items an execution service can take on.
      threshold: "90"
      # Frequency (in seconds) at which KEDA checks the metric value.
      pollingInterval: 15
      # Minimum number of pods to maintain, even if the load is low
      minReplicaCount: 2
      # Maximum number of pods to scale up to when the load increases.
      maxReplicaCount: 5
  # Liveness probe checks if the app is still running. 
  # If it fails, the pod will be restarted.
  livenessProbe:
    periodSeconds: 10
    failureThreshold: 3
    timeoutSeconds: 2
  # Readiness probe checks if the app is ready to receive traffic. 
  # If it fails, traffic is paused.
  readinessProbe:
    initialDelaySeconds: 5
    periodSeconds: 10
    failureThreshold: 3
    timeoutSeconds: 2
  # Startup probe checks if the app has finished starting.
  # Until it passes, liveness and readiness probes are ignored.
  startupProbe:
    failureThreshold: 60
    periodSeconds: 10
    timeoutSeconds: 2

# ========================== 
integrationapi:
  # If disabled, the integrationapi pod will not be deployed
  enabled: true
  # Number of pod replicas to deploy
  replicas: 1
  name: rpi-integrationapi
  enableSwagger: true
  serviceAccount: 
    enabled: true 
  enableRPIAuthentication: true
  # Default resource requests and limits per deployment.
  # These values are designed to fit 8 deployments on a single node with 8 vCPUs and 16 GB RAM.
  # Treat these as starting points and adjust accordingly to meet your utilization and performance requirements.
  resources:
    requests:
      cpu: 500m
      memory: 750Mi
    limits:
      cpu: 875m
      memory: 2048Mi
  service:
    # The port on which the Kubernetes services will be exposed.
    # This can be customized to align with the customer's internal port conventions.
    port: 80
  # Optional custom labels to apply to the Deployment and Pod metadata.
  # These will be added in addition to the default labels.
  # Example: environment: prod, team: marketing
  customLabels: 
    environment: "prod"
    team: "marketing"
  customAnnotations:
    my-custom-annotation: "my-value"
  customMetrics:
    # Enable or disable custom Prometheus metrics scraping for this service
    enabled: false
    # When enabled, the following annotation will be added to the pod metadata
    # to allow Prometheus scrape the /metrics endpoint
    prometheus_scrape: false
  # Supported options: Critical, Error, Warning, Information, Trace, Debug
  logging:
    default: Error
    database: Error
    rpiTrace: Error
    rpiError: Error
  # Enable Horizontal autoscaling
  autoscaling:
    enabled: true
    # Options: "keda" or "hpa".
    # 
    # - "keda": Enables KEDA (Kubernetes-based Event Driven Autoscaling), allowing scaling based on RPI custom metrics from
    #   Prometheus. This option assumes that KEDA is already installed in your cluster.
    #   If not, you can install it with Documentation: https://keda.sh/docs/latest/deploy/
    #
    # - "hpa": Enables the standard Kubernetes Horizontal Pod Autoscaler, which scales pods based on CPU and/or memory 
    #   usage metrics. This is natively supported by Kubernetes and does not require additional installation.
    #
    # Choose the appropriate option based on your scaling requirements. If you need scaling based on RPI custom metrics, use "keda".
    # For resource-based scaling, use "hpa".
    type: hpa
    minReplicas: 1
    maxReplicas: 5
    targetCPUUtilizationPercentage: 80
    # targetMemoryUtilizationPercentage: 80
    kedaScaledObject:
      # The Prometheus server address where metrics are queried.
      serverAddress: <my-prometheus-query-endpoint>
      # The name of the RPI custom metric KEDA will use to trigger scaling.
      metricName: <my-metrics-name>
      # The Prometheus query that fetches the metric value for scaling decisions.
      # This example filters the metric `execution_max_thread_count` based on:
      # - Namespace: "redpoint-rpi"
      # - Application: "rpi-executionservice"
      # - Azure Workload Identity: "true"
      #
      # Update these labels if your namespace, app, or identity settings are different.
      query: count(execution_max_thread_count{kubernetes_namespace="redpoint-rpi",app="rpi-executionservice",azure_workload_identity_use="true"})
      # Threshold for the value configured for `RPIExecution__MaxThreadsPerExecutionService`
      # It defines the limit on how many work items an execution service can take on.
      threshold: "90"
      # Frequency (in seconds) at which KEDA checks the metric value.
      pollingInterval: 15
      # Minimum number of pods to maintain, even if the load is low
      minReplicaCount: 2
      # Maximum number of pods to scale up to when the load increases.
      maxReplicaCount: 5
  # Liveness probe checks if the app is still running. 
  # If it fails, the pod will be restarted.
  livenessProbe:
    periodSeconds: 10
    failureThreshold: 3
    timeoutSeconds: 2
  # Readiness probe checks if the app is ready to receive traffic. 
  # If it fails, traffic is paused.
  readinessProbe:
    initialDelaySeconds: 5
    periodSeconds: 10
    failureThreshold: 3
    timeoutSeconds: 2
  # Startup probe checks if the app has finished starting.
  # Until it passes, liveness and readiness probes are ignored.
  startupProbe:
    failureThreshold: 60
    periodSeconds: 10
    timeoutSeconds: 2

# ========================== 
nodemanager:
  # If disabled, the nodemanager pod will not be deployed
  enabled: true
  # Number of pod replicas to deploy
  replicas: 1
  name: rpi-nodemanager
  serviceAccount: 
    enabled: true 
  enableRPIAuthentication: true
  enableSwagger: true
  # Default resource requests and limits per deployment.
  # These values are designed to fit 8 deployments on a single node with 8 vCPUs and 16 GB RAM.
  # Treat these as starting points and adjust accordingly to meet your utilization and performance requirements.
  resources:
    requests:
      cpu: 500m
      memory: 750Mi
    limits:
      cpu: 875m
      memory: 2048Mi
  service:
    # The port on which the Kubernetes services will be exposed.
    # This can be customized to align with the customer's internal port conventions.
    port: 80
  # Optional custom labels to apply to the Deployment and Pod metadata.
  # These will be added in addition to the default labels.
  # Example: environment: prod, team: marketing
  customLabels: 
    environment: "prod"
    team: "marketing"
  customAnnotations:
    my-custom-annotation: "my-value"
  customMetrics:
    # Enable or disable custom Prometheus metrics scraping for this service
    enabled: false
    # When enabled, the following annotation will be added to the pod metadata
    # to allow Prometheus scrape the /metrics endpoint
    prometheus_scrape: false
  # Set the minimum log level for application logging.
  # Supported options: Critical, Error, Warning, Information, Trace, Debug
  logging:
    default: Error
    database: Error
    rpiTrace: Error
    rpiError: Error
  # Enable Horizontal autoscaling
  autoscaling:
    enabled: true
    # Options: "keda" or "hpa".
    # 
    # - "keda": Enables KEDA (Kubernetes-based Event Driven Autoscaling), allowing scaling based on RPI custom metrics from
    #   Prometheus. This option assumes that KEDA is already installed in your cluster.
    #   If not, you can install it with Documentation: https://keda.sh/docs/latest/deploy/
    #
    # - "hpa": Enables the standard Kubernetes Horizontal Pod Autoscaler, which scales pods based on CPU and/or memory 
    #   usage metrics. This is natively supported by Kubernetes and does not require additional installation.
    #
    # Choose the appropriate option based on your scaling requirements. If you need scaling based on RPI custom metrics, use "keda".
    # For resource-based scaling, use "hpa".
    type: hpa
    minReplicas: 1
    maxReplicas: 5
    targetCPUUtilizationPercentage: 80
    # targetMemoryUtilizationPercentage: 80
    kedaScaledObject:
      # The Prometheus server address where metrics are queried.
      serverAddress: <my-prometheus-query-endpoint>
      # The name of the RPI custom metric KEDA will use to trigger scaling.
      metricName: <my-metrics-name>
      # The Prometheus query that fetches the metric value for scaling decisions.
      # This example filters the metric `execution_max_thread_count` based on:
      # - Namespace: "redpoint-rpi"
      # - Application: "rpi-executionservice"
      # - Azure Workload Identity: "true"
      #
      # Update these labels if your namespace, app, or identity settings are different.
      query: count(execution_max_thread_count{kubernetes_namespace="redpoint-rpi",app="rpi-executionservice",azure_workload_identity_use="true"})
      # Threshold for the value configured for `RPIExecution__MaxThreadsPerExecutionService`
      # It defines the limit on how many work items an execution service can take on.
      threshold: "90"
      # Frequency (in seconds) at which KEDA checks the metric value.
      pollingInterval: 15
      # Minimum number of pods to maintain, even if the load is low
      minReplicaCount: 2
      # Maximum number of pods to scale up to when the load increases.
      maxReplicaCount: 5
  # Liveness probe checks if the app is still running. 
  # If it fails, the pod will be restarted.
  livenessProbe:
    periodSeconds: 10
    failureThreshold: 3
    timeoutSeconds: 2
  # Readiness probe checks if the app is ready to receive traffic. 
  # If it fails, traffic is paused.
  readinessProbe:
    initialDelaySeconds: 5
    periodSeconds: 10
    failureThreshold: 3
    timeoutSeconds: 2
  # Startup probe checks if the app has finished starting.
  # Until it passes, liveness and readiness probes are ignored.
  startupProbe:
    failureThreshold: 60
    periodSeconds: 10
    timeoutSeconds: 2

# ========================== 
deploymentapi:
  # If disabled, the deploymentapi pod will not be deployed
  enabled: true
  # Number of pod replicas to deploy
  replicas: 1
  name: rpi-deploymentapi
  serviceAccount: 
    enabled: true 
  # Default resource requests and limits per deployment.
  # These values are designed to fit 8 deployments on a single node with 8 vCPUs and 16 GB RAM.
  # Treat these as starting points and adjust accordingly to meet your utilization and performance requirements.
  resources:
    requests:
      cpu: 500m
      memory: 750Mi
    limits:
      cpu: 875m
      memory: 2048Mi
  service:
    # The port on which the Kubernetes services will be exposed.
    # This can be customized to align with the customer's internal port conventions.
    port: 80
  # Optional custom labels to apply to the Deployment and Pod metadata.
  # These will be added in addition to the default labels.
  # Example: environment: prod, team: marketing
  customLabels: 
    environment: "prod"
    team: "marketing"
  customAnnotations:
    my-custom-annotation: "my-value"
  customMetrics:
    # Enable or disable custom Prometheus metrics scraping for this service
    enabled: false
    # When enabled, the following annotation will be added to the pod metadata
    # to allow Prometheus scrape the /metrics endpoint
    prometheus_scrape: false
  # Set the minimum log level for application logging.
  # Supported options: Critical, Error, Warning, Information, Trace, Debug
  logging:
    default: Error
    database: Error
    rpiTrace: Error
    rpiError: Error

# ==========================
queuereader: 
  # If disabled, the queuereader pod will not be deployed
  enabled: true
  # Number of pod replicas to deploy
  replicas: 1
  name: rpi-queuereader
  # Configuration for the Queue Reader container introduced in RPI v7.4
  # This component handles the draining of Queue Listener and RPI Realtime queues.
  serviceAccount: 
    enabled: true 
  # Enable or disable processing for specific queues
  isFormProcessingEnabled: true
  isEventProcessingEnabled: true
  isCacheProcessingEnabled: true
  queueListenerEnabled: true
  # Queue for storing messages received by inactive triggers.
  # You can customize the queue name as needed.
  listenerQueueNonActiveQueuePath: listenerQueueNonActive
  # Time-to-live (TTL) in days for messages in the inactive trigger queue.
  listenerQueueNonActiveTTLDays: 14
  # Queue for storing messages that encountered errors during processing.
  # You can customize the queue name as needed.
  listenerQueueErrorQueuePath: listenerQueueError
  # Time-to-live (TTL) in days for messages in the error queue.
  listenerQueueErrorTTLDays: 14
  realtimeConfiguration:
    # Distribution mode for high-performance or high-volume transactions
    # Set to true if you require distributed processing
    isDistributed: false
    distributedCache:
      # Only Redis is currently supported
      provider: Redis
      connectionString: <my-redis-connection-string>
    # Comma separated List of RPI client IDs associated with your RPI cluster 
    tenantIds:
      - 00000000-0000-0000-0000-000000000000
      - 11111111-1111-1111-1111-111111111111
  # Thread pool configuration
  threadPoolSize: 10
  # Timeout duration for processing, in minutes
  timeoutMinutes: 60
  # Maximum number of messages to process in a single batch
  maxBatchSize: 50
  useMessageLocks: true
  resources:
    requests:
      cpu: 500m
      memory: 750Mi
    limits:
      cpu: 875m
      memory: 2048Mi
  service:
    # The port on which the Kubernetes services will be exposed.
    # This can be customized to align with the customer's internal port conventions.
    port: 80
  # Optional custom labels to apply to the Deployment and Pod metadata.
  # These will be added in addition to the default labels.
  # Example: environment: prod, team: marketing
  customLabels: 
    environment: "prod"
    team: "marketing"
  customAnnotations:
    my-custom-annotation: "my-value"
  customMetrics:
    # Enable or disable custom Prometheus metrics scraping for this service
    enabled: false
    # When enabled, the following annotation will be added to the pod metadata
    # to allow Prometheus scrape the /metrics endpoint
    prometheus_scrape: false
  # Set the minimum log level for application logging.
  # Supported options: Critical, Error, Warning, Information, Trace, Debug
  logging:
    default: Error
    database: Error
    rpiTrace: Error
    rpiError: Error
  # Enable Horizontal autoscaling
  autoscaling:
    enabled: true
    # Options: "keda" or "hpa".
    # 
    # - "keda": Enables KEDA (Kubernetes-based Event Driven Autoscaling), allowing scaling based on RPI custom metrics from
    #   Prometheus. This option assumes that KEDA is already installed in your cluster.
    #   If not, you can install it with Documentation: https://keda.sh/docs/latest/deploy/
    #
    # - "hpa": Enables the standard Kubernetes Horizontal Pod Autoscaler, which scales pods based on CPU and/or memory 
    #   usage metrics. This is natively supported by Kubernetes and does not require additional installation.
    #
    # Choose the appropriate option based on your scaling requirements. If you need scaling based on RPI custom metrics, use "keda".
    # For resource-based scaling, use "hpa".
    type: hpa
    minReplicas: 1
    maxReplicas: 5
    targetCPUUtilizationPercentage: 80
    # targetMemoryUtilizationPercentage: 80
    kedaScaledObject:
      # The Prometheus server address where metrics are queried.
      serverAddress: <my-prometheus-query-endpoint>
      # The name of the RPI custom metric KEDA will use to trigger scaling.
      metricName: <my-metrics-name>
      # The Prometheus query that fetches the metric value for scaling decisions.
      # This example filters the metric `execution_max_thread_count` based on:
      # - Namespace: "redpoint-rpi"
      # - Application: "rpi-executionservice"
      # - Azure Workload Identity: "true"
      #
      # Update these labels if your namespace, app, or identity settings are different.
      query: count(execution_max_thread_count{kubernetes_namespace="redpoint-rpi",app="rpi-executionservice",azure_workload_identity_use="true"})
      # Threshold for the value configured for `RPIExecution__MaxThreadsPerExecutionService`
      # It defines the limit on how many work items an execution service can take on.
      threshold: "90"
      # Frequency (in seconds) at which KEDA checks the metric value.
      pollingInterval: 15
      # Minimum number of pods to maintain, even if the load is low
      minReplicaCount: 2
      # Maximum number of pods to scale up to when the load increases.
      maxReplicaCount: 5
  # Liveness probe checks if the app is still running. 
  # If it fails, the pod will be restarted.
  livenessProbe:
    periodSeconds: 10
    failureThreshold: 3
    timeoutSeconds: 2
  # Readiness probe checks if the app is ready to receive traffic. 
  # If it fails, traffic is paused.
  readinessProbe:
    initialDelaySeconds: 5
    periodSeconds: 10
    failureThreshold: 3
    timeoutSeconds: 2
  # Startup probe checks if the app has finished starting.
  # Until it passes, liveness and readiness probes are ignored.
  startupProbe:
    failureThreshold: 60
    periodSeconds: 10
    timeoutSeconds: 2

OpenIdProviders:
  # Enable OpenID Connect settings
  enabled: false
  # Name of the OpenID Connect provider. Supported Providers 
  # - KeyCloak
  # - Okta
  # - AzureAD
  # https://docs.redpointglobal.com/rpi/admin-authentication
  name: AzureAD
  # Microsoft Entra ID App registration Client ID
  # Authorization host for OpenID Connect
  authorizationHost: https://login.microsoftonline.com/00000000-0000-0000-0000-000000000000/v2.0
  clientID: 00000000-0000-0000-0000-000000000000
  # Audience for the OpenID Connect authentication request
  audience: api://00000000-0000-0000-0000-000000000000
  redirectURL: https://rpi-interactionapi.example.com
  enableRefreshTokens: true
  # Validate issuer of the OpenID Connect provider
  validateIssuer: true
  # Enable or disable refresh tokens
  # Validate audience of the OpenID Connect provider
  validateAudience: true
  # Parameter for id_token_hint during logout
  logoutIdTokenParameter: id_token_hint
  # Custom scopes for OpenID Connect
  customScopes: api://00000000-0000-0000-0000-000000000000/Interaction.Clients

SMTPSettings:
  SMTP_SenderAddress: noreply-rpi@example.com
  SMTP_Address: your_smtp_host
  SMTP_Port: 587
  EnableSSL: true
  UseCredentials: true
  SMTP_Username: your_smtp_server_username
  SMTP_Password: your_smtp_server_password

# ========================== 
ingress:
  controller:
    # Set enabled to false if you want to disable the creation of the ingress controller
    enabled: true 
  # Set mode to internal for private ingress and public for public ingress
  mode: public 
  # Set certificateSource to acm if your certificate is managed in AWS Certificate Manager 
  certificateSource: kubernetes # acm
  tlsSecretName: ingress-tls
  className: nginx-redpoint-rpi
  # Subnet name is only required if you set the ingress mode to private
  subnetName: <my-ingress-vpc-subnet-name> 
  # Certificate Arn is only required if you set the certificateSource to acm
  certificateArn: your_acm_certificate_arn
  # Specify the domain name for the ingress resources.
  domain: example.com
  # Add any specific annotations required for your ingress setup here.
  annotations:
    nginx.ingress.kubernetes.io/proxy-body-size: 99m
    nginx.ingress.kubernetes.io/proxy-read-timeout: "3600"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "3600"
  customLabels: 
    environment: prod
    team: marketing
  service:
    port: 80
  customAnnotations:
    prometheus.io/scrape: "false"
  # Define hostnames for different services in your application.
  hosts:
    # Configuration service
    config: rpi-deploymentapi
    # Client Login service
    client: rpi-interactionapi
    # Integration API
    integration: rpi-integrationapi
    # Real-time service 
    realtime: rpi-realtimeapi
    # Callback API
    callbackapi: rpi-callbackapi
    # Queue Reader Operational endpoints
    queuereader: rpi-queuereader

securityContext:
  # Enable this to enforce running the container as a non-root user.
  enabled: true
  # Indicates whether a custom user/group is being provided.
  # If false, the default 7777 will be used
  # We highly recommend you dont change these settings and let it use the defaults).
  isCustom: true
  runAsUser: 7777
  runAsGroup: 7777
  fsGroup: 7777

# NodeSelector is used to control scheduling by specifying node labels.
# When enabled, the deployment will only be scheduled on nodes that match the provided key-value pair.
nodeSelector:
  enabled: false
  key: app
  value: redpoint-rpi

# Tolerations allow the deployment to be scheduled on tainted nodes.
# When enabled, this ensures that the workload can run on nodes explicitly reserved for Redpoint RPI by tolerating their taints.
tolerations:
  enabled: false
  effect: NoSchedule
  key: app
  operator: Equal
  value: redpoint-rpi

# ==========================
redpointAI:
  # Set to true to enable Redpoint AI features
  enabled: false  
  naturalLanguage:
    # your OpenAI API key
    ApiKey: <my-openai-key>
    # Base URL for the OpenAI endpoint             
    ApiBase: https://example.openai.azure.com/
    # API version (e.g., 2023-05-15)            
    ApiVersion: 2023-07-01-preview
    # Name of the deployed ChatGPT engine (e.g., gpt-35-turbo)           
    ChatGptEngine: gpt-4-32k
    # Temperature for ChatGPT responses (e.g., 0.7)        
    ChatGptTemp: 0.5
  cognitiveSearch:
    # Azure Cognitive Search endpoint URL
    SearchEndpoint: https://example.search.windows.net
    # API key for Azure Cognitive Search       
    SearchKey: <my-cognitivesearch-key>
    # Azure Cognitive Search vector profile name    
    VectorSearchProfile: vector-profile-000000000000
    # Azure Cognitive Search vector config name 
    VectorSearchConfig:  vector-config-000000000000 
  modelStorage:
    # Azure Blob Storage connection string for model artifacts
    ConnectionString: DefaultEndpointsProtocol=https;AccountName=my_account_name;AccountKey=my_account_access_key 
    # Name of the embeddings model (e.g., text-embedding-ada-002)
    EmbeddingsModel: text-embedding-ada-002
    # Dimensionality of the embeddings (e.g., 1536)            
    ModelDimensions: 1536 
    # Name of the Azure Blob Storage container holding vector index data
    ContainerName: my_blob_container_name
    # Path inside the container where model files are stored       
    BlobFolder: my_blob_container_folder_name
